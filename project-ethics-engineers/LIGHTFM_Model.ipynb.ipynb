{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting the all-user Data + Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sana/anaconda3/envs/spyder-env/lib/python3.11/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "from lightfm.data import Dataset\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import scipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting the Data + Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Impression ID', 'User ID', 'Time', 'History', 'Impressions']\n",
    "behaviors_df = pd.read_csv(\"MINDsmall_train/behaviors.tsv\", sep='\\t', names=column_names)\n",
    "\n",
    "behaviors_df['Impressions'] = behaviors_df['Impressions'].str.split(' ')\n",
    "behaviors_df = behaviors_df.explode('Impressions').rename(columns={'Impressions': 'News_ID'})\n",
    "behaviors_df[['News_ID', 'News_Subcategory']] = behaviors_df['News_ID'].str.split('-', n=1, expand=True)\n",
    "behaviors_df['News_ID'] = behaviors_df['News_ID'].astype(str)\n",
    "\n",
    "\n",
    "news_column_names = ['News_ID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entities']\n",
    "news_df = pd.read_csv(\"MINDsmall_train/news.tsv\", sep='\\t', names=news_column_names)\n",
    "news_df['News_ID'] = news_df['News_ID'].astype(str)\n",
    "\n",
    "merged_impression_df = behaviors_df.merge(news_df, on='News_ID', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Preparing all-user Data for Modeling: category+subcategory predictors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Item Features: 100%|██████████| 4988584/4988584 [00:09<00:00, 539757.72it/s]\n",
      "Building User Features: 100%|██████████| 49108/49108 [00:00<00:00, 369069.95it/s]\n",
      "Building Interactions: 100%|██████████| 4988584/4988584 [00:05<00:00, 952879.07it/s] \n"
     ]
    }
   ],
   "source": [
    "grouped_df = merged_impression_df.groupby(['User ID', 'News_ID']).agg({\n",
    "    'News_Subcategory': 'sum', \n",
    "    'Category': 'first', \n",
    "    'SubCategory': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df['News_Subcategory'] = grouped_df['News_Subcategory'].astype(float)\n",
    "merged_impression_df['History'] = merged_impression_df['History'].apply(lambda x: x if isinstance(x, str) and x.strip() else None)\n",
    "\n",
    "valid_histories = merged_impression_df['History'].dropna().unique()\n",
    "user_features_list = [f\"history_{history}\" for history in valid_histories]\n",
    "user_features_data = [\n",
    "    (user_id, [f\"history_{history}\"])\n",
    "    for user_id, history in merged_impression_df.groupby('User ID')['History'].first().items()\n",
    "    if history is not None\n",
    "]\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=grouped_df['User ID'].unique(),\n",
    "    items=grouped_df['News_ID'].unique(),\n",
    "    user_features=user_features_list,\n",
    "    item_features=list(set(grouped_df['Category']) | set(grouped_df['SubCategory']))\n",
    ")\n",
    "\n",
    "item_features = dataset.build_item_features(\n",
    "    tqdm([(row['News_ID'], [row['Category'], row['SubCategory']]) for _, row in grouped_df.iterrows()], desc=\"Building Item Features\")\n",
    ")\n",
    "user_features = dataset.build_user_features(\n",
    "    tqdm(user_features_data, desc=\"Building User Features\")\n",
    ")\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    tqdm([(row['User ID'], row['News_ID'], float(row['News_Subcategory'])) for _, row in grouped_df.iterrows()], desc=\"Building Interactions\")\n",
    ")\n",
    "\n",
    "interactions = interactions.astype('float32')\n",
    "weights = weights.astype('float32')\n",
    "item_features = coo_matrix(item_features)\n",
    "user_features = coo_matrix(user_features)\n",
    "\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2) \n",
    "train_weights = weights.multiply(train > 0)\n",
    "train_weights = coo_matrix(train_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Finding optimal parameters: Use category+Subcategory predictors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_range = [10, 20, 30]\n",
    "loss_range = ['warp', 'bpr', 'warp-kos']\n",
    "epoch_range = [10, 20, 30]\n",
    "num_threads = 4\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_components, loss, epochs in product(n_components_range, loss_range, epoch_range):\n",
    "    print(f\"Training model with n_components={n_components}, loss={loss}, epochs={epochs}\")\n",
    "\n",
    "    model = LightFM(no_components=n_components, loss=loss)\n",
    "    model.fit(\n",
    "        train,\n",
    "        item_features=item_features,\n",
    "        epochs=epochs,\n",
    "        num_threads=num_threads,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    train_auc = auc_score(model, train, item_features=item_features).mean()\n",
    "    test_auc = auc_score(model, test, item_features=item_features).mean()\n",
    "    \n",
    "    # Save the results\n",
    "    results.append({\n",
    "        'n_components': n_components,\n",
    "        'loss': loss,\n",
    "        'epochs': epochs,\n",
    "        'train_auc': train_auc,\n",
    "        'test_auc': test_auc\n",
    "    })\n",
    "\n",
    "results = sorted(results, key=lambda x: x['test_auc'], reverse=True)\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x1817a8910>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model\n",
    "model = LightFM(no_components=30, loss='warp')  \n",
    "\n",
    "model.fit(\n",
    "    interactions=train, \n",
    "    user_features=user_features,  \n",
    "    item_features=item_features, \n",
    "    epochs=30,  \n",
    "    num_threads=4  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96387655"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_train = auc_score(model, train, user_features=user_features, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92696345"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_test = auc_score(model, test, user_features=user_features, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating Mrr\n",
    "def calculate_mrr(model, interactions, item_features=None):\n",
    "    if not isinstance(interactions, csr_matrix):\n",
    "        interactions = csr_matrix(interactions)\n",
    "\n",
    "    mrr = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].indices  \n",
    "        ranked_items = np.argsort(-scores)\n",
    "        ranks = np.where(np.isin(ranked_items, true_items))[0]\n",
    "        if len(ranks) > 0:\n",
    "            mrr.append(1.0 / (ranks[0] + 1))  \n",
    "    return np.mean(mrr)\n",
    "\n",
    "\n",
    "train = csr_matrix(train) \n",
    "test = csr_matrix(test)\n",
    "train_mrr = calculate_mrr(model, train, item_features=item_features)\n",
    "test_mrr = calculate_mrr(model, test, item_features=item_features)\n",
    "\n",
    "print(f\"Train MRR: {train_mrr:.4f}\")\n",
    "print(f\"Test MRR: {test_mrr:.4f}\")\n",
    "\n",
    "\n",
    "#Calculating ndcg@5 and ndcg@10\n",
    "def calculate_ndcg(model, interactions, k, item_features=None):\n",
    "    if not isinstance(interactions, scipy.sparse.csr_matrix):\n",
    "        interactions = interactions.tocsr()\n",
    "    \n",
    "    ndcg = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].toarray().flatten()  \n",
    "        ranked_items = np.argsort(-scores)[:k]\n",
    "   \n",
    "        dcg = sum(\n",
    "            (true_items[item] / np.log2(rank + 2))\n",
    "            for rank, item in enumerate(ranked_items)\n",
    "            if true_items[item] > 0\n",
    "        )\n",
    "      \n",
    "        sorted_true_items = np.sort(true_items)[::-1][:k]\n",
    "        idcg = sum(\n",
    "            (rel / np.log2(rank + 2))\n",
    "            for rank, rel in enumerate(sorted_true_items)\n",
    "            if rel > 0\n",
    "        )\n",
    "        ndcg.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "    return np.mean(ndcg)\n",
    "\n",
    "\n",
    "ndcg_at_5_train = calculate_ndcg(model, train, k=5, item_features=item_features)\n",
    "ndcg_at_10_train = calculate_ndcg(model, train, k=10, item_features=item_features)\n",
    "\n",
    "ndcg_at_5_test = calculate_ndcg(model, test, k=5, item_features=item_features)\n",
    "ndcg_at_10_test = calculate_ndcg(model, test, k=10, item_features=item_features)\n",
    "\n",
    "print(f\"Train nDCG@5: {ndcg_at_5_train:.4f}, Train nDCG@10: {ndcg_at_10_train:.4f}\")\n",
    "print(f\"Test nDCG@5: {ndcg_at_5_test:.4f}, Test nDCG@10: {ndcg_at_10_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Preparing all-user Data for Modeling: category+subcategory+Titles predictors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building User Features: 100%|██████████| 49108/49108 [00:00<00:00, 382169.85it/s]\n",
      "Building Interactions: 100%|██████████| 4988584/4988584 [00:05<00:00, 907533.74it/s] \n"
     ]
    }
   ],
   "source": [
    "grouped_df = merged_impression_df.groupby(['User ID', 'News_ID']).agg({\n",
    "    'News_Subcategory': 'sum', \n",
    "    'Category': 'first',      \n",
    "    'SubCategory': 'first',    \n",
    "    'Title': 'first',          \n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "grouped_df['News_Subcategory'] = grouped_df['News_Subcategory'].astype(float)\n",
    "merged_impression_df['History'] = merged_impression_df['History'].apply(lambda x: x if isinstance(x, str) and x.strip() else None)\n",
    "valid_histories = merged_impression_df['History'].dropna().unique()\n",
    "user_features_list = [f\"history_{history}\" for history in valid_histories]\n",
    "\n",
    "user_features_data = [\n",
    "    (user_id, [f\"history_{history}\"])\n",
    "    for user_id, history in merged_impression_df.groupby('User ID')['History'].first().items()\n",
    "    if history is not None\n",
    "]\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=grouped_df['User ID'].unique(),\n",
    "    items=grouped_df['News_ID'].unique(),\n",
    "    user_features=user_features_list,\n",
    "    item_features=list(set(grouped_df['Category']) | set(grouped_df['SubCategory']) | set(grouped_df['Title']))\n",
    ")\n",
    "\n",
    "item_features = dataset.build_item_features([\n",
    "    (\n",
    "        row['News_ID'],\n",
    "        [row['Category'], row['SubCategory'], row['Title']]\n",
    "    )\n",
    "    for _, row in grouped_df.iterrows()\n",
    "])\n",
    "\n",
    "\n",
    "user_features = dataset.build_user_features(\n",
    "    tqdm(user_features_data, desc=\"Building User Features\")\n",
    ")\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    tqdm([(row['User ID'], row['News_ID'], float(row['News_Subcategory'])) for _, row in grouped_df.iterrows()], desc=\"Building Interactions\")\n",
    ")\n",
    "\n",
    "\n",
    "interactions = interactions.astype('float32')\n",
    "weights = weights.astype('float32')\n",
    "item_features = coo_matrix(item_features)\n",
    "user_features = coo_matrix(user_features)\n",
    "\n",
    "\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2) \n",
    "train_weights = weights.multiply(train > 0)\n",
    "train_weights = coo_matrix(train_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x1ca9c9e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model\n",
    "model = LightFM(no_components=30, loss='warp')  \n",
    "\n",
    "model.fit(\n",
    "    interactions=train, \n",
    "    user_features=user_features,  \n",
    "    item_features=item_features, \n",
    "    epochs=30,  \n",
    "    num_threads=4  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943037"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_train = auc_score(model, train,  user_features=user_features, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9882977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_test = auc_score(model, test, user_features=user_features, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating Mrr\n",
    "def calculate_mrr(model, interactions, item_features=None):\n",
    "    if not isinstance(interactions, csr_matrix):\n",
    "        interactions = csr_matrix(interactions)\n",
    "\n",
    "    mrr = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].indices  \n",
    "        ranked_items = np.argsort(-scores)\n",
    "        ranks = np.where(np.isin(ranked_items, true_items))[0]\n",
    "        if len(ranks) > 0:\n",
    "            mrr.append(1.0 / (ranks[0] + 1))  \n",
    "    return np.mean(mrr)\n",
    "\n",
    "\n",
    "train = csr_matrix(train) \n",
    "test = csr_matrix(test)\n",
    "train_mrr = calculate_mrr(model, train, item_features=item_features)\n",
    "test_mrr = calculate_mrr(model, test, item_features=item_features)\n",
    "\n",
    "print(f\"Train MRR: {train_mrr:.4f}\")\n",
    "print(f\"Test MRR: {test_mrr:.4f}\")\n",
    "\n",
    "\n",
    "#Calculating ndcg@5 and ndcg@10\n",
    "def calculate_ndcg(model, interactions, k, item_features=None):\n",
    "    if not isinstance(interactions, scipy.sparse.csr_matrix):\n",
    "        interactions = interactions.tocsr()\n",
    "    \n",
    "    ndcg = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].toarray().flatten()  \n",
    "        ranked_items = np.argsort(-scores)[:k]\n",
    "   \n",
    "        dcg = sum(\n",
    "            (true_items[item] / np.log2(rank + 2))\n",
    "            for rank, item in enumerate(ranked_items)\n",
    "            if true_items[item] > 0\n",
    "        )\n",
    "      \n",
    "        sorted_true_items = np.sort(true_items)[::-1][:k]\n",
    "        idcg = sum(\n",
    "            (rel / np.log2(rank + 2))\n",
    "            for rank, rel in enumerate(sorted_true_items)\n",
    "            if rel > 0\n",
    "        )\n",
    "        ndcg.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "    return np.mean(ndcg)\n",
    "\n",
    "\n",
    "ndcg_at_5_train = calculate_ndcg(model, train, k=5, item_features=item_features)\n",
    "ndcg_at_10_train = calculate_ndcg(model, train, k=10, item_features=item_features)\n",
    "\n",
    "ndcg_at_5_test = calculate_ndcg(model, test, k=5, item_features=item_features)\n",
    "ndcg_at_10_test = calculate_ndcg(model, test, k=10, item_features=item_features)\n",
    "\n",
    "print(f\"Train nDCG@5: {ndcg_at_5_train:.4f}, Train nDCG@10: {ndcg_at_10_train:.4f}\")\n",
    "print(f\"Test nDCG@5: {ndcg_at_5_test:.4f}, Test nDCG@10: {ndcg_at_10_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No-User History: Cleaning and Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Impression ID', 'User ID', 'Time', 'History', 'Impressions',]\n",
    "behaviors_df = pd.read_csv(\"MINDsmall_train/behaviors.tsv\", sep='\\t', names=column_names)\n",
    "behaviors_df['History'] = behaviors_df['History'].str.split(' ')\n",
    "behaviors_df = behaviors_df.explode('History').rename(columns={'History': 'News_ID'})\n",
    "\n",
    "\n",
    "news_column_names = ['News_ID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title Entities', 'Abstract Entities']\n",
    "news_df = pd.read_csv(\"MINDsmall_train/news.tsv\", sep='\\t', names=news_column_names)\n",
    "merged_df = behaviors_df.merge(news_df, on='News_ID', how='left')\n",
    "merged_df = merged_df.drop_duplicates(subset=[\"User ID\",'News_ID'])\n",
    "\n",
    "\n",
    "\n",
    "Nan_df = merged_df[merged_df[\"News_ID\"].isna()]\n",
    "Nan_df1 = Nan_df.assign(Impressions=Nan_df['Impressions'].str.split()).explode('Impressions')\n",
    "Nan_df1['News_ID'] = Nan_df1['Impressions'].str.split('-').str[0]\n",
    "Nan_df1['Impressions'] = Nan_df1['Impressions'].str.split('-').str[1]\n",
    "\n",
    "merged_null_df = Nan_df1.merge(news_df, on='News_ID', how='left')\n",
    "merged_null_df = merged_null_df.loc[:, ~merged_null_df.columns.str.endswith('_x')]\n",
    "Nan_df = merged_df[merged_df[\"News_ID\"].isna()]\n",
    "Nan_df1 = Nan_df.assign(Impressions=Nan_df['Impressions'].str.split()).explode('Impressions')\n",
    "\n",
    "Nan_df1['News_ID'] = Nan_df1['Impressions'].str.split('-').str[0]\n",
    "Nan_df1['Impressions'] = Nan_df1['Impressions'].str.split('-').str[1]\n",
    "\n",
    "\n",
    "merged_null_df = Nan_df1.merge(news_df, on='News_ID', how='left')\n",
    "merged_null_df = merged_null_df.loc[:, ~merged_null_df.columns.str.endswith('_x')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Preparing no history user Data for Modeling: category+subcategory predictors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = merged_null_df.groupby(['User ID', 'News_ID']).agg({\n",
    "    'Impressions': 'sum',  \n",
    "    'Category_y': 'first',  \n",
    "    'SubCategory_y': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "grouped_df['Impressions'] = grouped_df['Impressions'].astype(float)\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=grouped_df['User ID'].unique(),\n",
    "    items=grouped_df['News_ID'].unique(),\n",
    "    user_features=None, \n",
    "    item_features=list(set(grouped_df['Category_y']) | set(grouped_df['SubCategory_y']))\n",
    ")\n",
    "item_features = dataset.build_item_features(\n",
    "    [(row['News_ID'], [str(row['Category_y']), str(row['SubCategory_y'])]) for _, row in grouped_df.iterrows()]\n",
    ")\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    [(row['User ID'], row['News_ID'], float(row['Impressions'])) for _, row in grouped_df.iterrows()]\n",
    ")\n",
    "\n",
    "\n",
    "interactions = interactions.astype('float32')\n",
    "weights = weights.astype('float32')\n",
    "item_features = coo_matrix(item_features)\n",
    "\n",
    "\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2) \n",
    "train_weights = weights.multiply(train > 0) \n",
    "train_weights = coo_matrix(train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:00<00:00, 63.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x185b23d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model\n",
    "n_components =10\n",
    "loss = 'warp'\n",
    "epoch = 30\n",
    "num_thread = 4\n",
    "model = LightFM(no_components=n_components, loss=loss)\n",
    "\n",
    "model.fit(\n",
    "    train,\n",
    "    item_features=item_features,\n",
    "    epochs=epoch,\n",
    "    num_threads=num_thread,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89423525"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_train = auc_score(model, train, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8077622"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_test = auc_score(model, test,item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MRR: 0.3842\n",
      "Test MRR: 0.0861\n",
      "Train nDCG@5: 0.1895, Train nDCG@10: 0.1749\n",
      "Test nDCG@5: 0.0290, Test nDCG@10: 0.0306\n"
     ]
    }
   ],
   "source": [
    "#Calculating mrr \n",
    "def calculate_mrr(model, interactions, item_features=None):\n",
    "    if not isinstance(interactions, csr_matrix):\n",
    "        interactions = csr_matrix(interactions)\n",
    "\n",
    "    mrr = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].indices  \n",
    "        ranked_items = np.argsort(-scores)\n",
    "        ranks = np.where(np.isin(ranked_items, true_items))[0]\n",
    "        if len(ranks) > 0:\n",
    "            mrr.append(1.0 / (ranks[0] + 1))  \n",
    "    return np.mean(mrr)\n",
    "\n",
    "\n",
    "train = csr_matrix(train) \n",
    "test = csr_matrix(test)\n",
    "train_mrr = calculate_mrr(model, train, item_features=item_features)\n",
    "test_mrr = calculate_mrr(model, test, item_features=item_features)\n",
    "\n",
    "print(f\"Train MRR: {train_mrr:.4f}\")\n",
    "print(f\"Test MRR: {test_mrr:.4f}\")\n",
    "\n",
    "\n",
    "#Calculating ndcg@5 and ndcg@10\n",
    "\n",
    "def calculate_ndcg(model, interactions, k, item_features=None):\n",
    "    if not isinstance(interactions, scipy.sparse.csr_matrix):\n",
    "        interactions = interactions.tocsr()\n",
    "    \n",
    "    ndcg = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].toarray().flatten()  \n",
    "        ranked_items = np.argsort(-scores)[:k]\n",
    "   \n",
    "        dcg = sum(\n",
    "            (true_items[item] / np.log2(rank + 2))\n",
    "            for rank, item in enumerate(ranked_items)\n",
    "            if true_items[item] > 0\n",
    "        )\n",
    "      \n",
    "        sorted_true_items = np.sort(true_items)[::-1][:k]\n",
    "        idcg = sum(\n",
    "            (rel / np.log2(rank + 2))\n",
    "            for rank, rel in enumerate(sorted_true_items)\n",
    "            if rel > 0\n",
    "        )\n",
    "        ndcg.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "    return np.mean(ndcg)\n",
    "\n",
    "\n",
    "ndcg_at_5_train = calculate_ndcg(model, train, k=5, item_features=item_features)\n",
    "ndcg_at_10_train = calculate_ndcg(model, train, k=10, item_features=item_features)\n",
    "\n",
    "ndcg_at_5_test = calculate_ndcg(model, test, k=5, item_features=item_features)\n",
    "ndcg_at_10_test = calculate_ndcg(model, test, k=10, item_features=item_features)\n",
    "\n",
    "print(f\"Train nDCG@5: {ndcg_at_5_train:.4f}, Train nDCG@10: {ndcg_at_10_train:.4f}\")\n",
    "print(f\"Test nDCG@5: {ndcg_at_5_test:.4f}, Test nDCG@10: {ndcg_at_10_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Preparing no history user Data for Modeling: category+subcategory+Titles predictors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Item Features: 100%|██████████| 33621/33621 [00:00<00:00, 449227.94it/s]\n",
      "Building Interactions: 100%|██████████| 33621/33621 [00:00<00:00, 896488.18it/s]\n"
     ]
    }
   ],
   "source": [
    "grouped_df = merged_null_df.groupby(['User ID', 'News_ID']).agg({\n",
    "    'Impressions': 'sum',  \n",
    "    'Category_y': 'first',        \n",
    "    'SubCategory_y': 'first',    \n",
    "    'Title_y': 'first',     \n",
    "}).reset_index()\n",
    "grouped_df['Impressions'] = grouped_df['Impressions'].astype(float)\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=grouped_df['User ID'].unique(),\n",
    "    items=grouped_df['News_ID'].unique(),\n",
    "    item_features=list(set(grouped_df['Category_y']) | set(grouped_df['SubCategory_y']) | set(grouped_df['Title_y']))\n",
    ")\n",
    "item_features = dataset.build_item_features(\n",
    "    tqdm([\n",
    "        (\n",
    "            row['News_ID'],\n",
    "            [row['Category_y'], row['SubCategory_y'], row['Title_y']]\n",
    "        )\n",
    "        for _, row in grouped_df.iterrows()\n",
    "    ], desc=\"Building Item Features\")\n",
    ")\n",
    "\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    tqdm([(row['User ID'], row['News_ID'], float(row['Impressions'])) for _, row in grouped_df.iterrows()], desc=\"Building Interactions\")\n",
    ")\n",
    "interactions = interactions.astype('float32')\n",
    "weights = weights.astype('float32')\n",
    "item_features = coo_matrix(item_features)\n",
    "\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2) \n",
    "train_weights = weights.multiply(train > 0)  \n",
    "train_weights = coo_matrix(train_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [00:01<00:00, 29.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x1816e0590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting model\n",
    "n_components =30\n",
    "loss = 'warp'\n",
    "epoch = 30\n",
    "num_thread = 4\n",
    "model = LightFM(no_components=n_components, loss=loss)\n",
    "\n",
    "model.fit(\n",
    "    train,\n",
    "    item_features=item_features,\n",
    "    epochs=epoch,\n",
    "    num_threads=num_thread,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9896907"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_train = auc_score(model, train, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_test = auc_score(model, test, item_features=item_features, num_threads=4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MRR: 0.8340\n",
      "Test MRR: 0.2530\n",
      "Train nDCG@5: 0.7002, Train nDCG@10: 0.6925\n",
      "Test nDCG@5: 0.1158, Test nDCG@10: 0.1499\n"
     ]
    }
   ],
   "source": [
    "#calculating mrr\n",
    "def calculate_mrr(model, interactions, item_features=None):\n",
    "    if not isinstance(interactions, csr_matrix):\n",
    "        interactions = csr_matrix(interactions)\n",
    "\n",
    "    mrr = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].indices  \n",
    "        ranked_items = np.argsort(-scores)\n",
    "        ranks = np.where(np.isin(ranked_items, true_items))[0]\n",
    "        if len(ranks) > 0:\n",
    "            mrr.append(1.0 / (ranks[0] + 1))  \n",
    "    return np.mean(mrr)\n",
    "\n",
    "\n",
    "train = csr_matrix(train) \n",
    "test = csr_matrix(test)\n",
    "train_mrr = calculate_mrr(model, train, item_features=item_features)\n",
    "test_mrr = calculate_mrr(model, test, item_features=item_features)\n",
    "\n",
    "print(f\"Train MRR: {train_mrr:.4f}\")\n",
    "print(f\"Test MRR: {test_mrr:.4f}\")\n",
    "\n",
    "\n",
    "#Calculating ndcg@5 and ndcg@10\n",
    "def calculate_ndcg(model, interactions, k, item_features=None):\n",
    "    if not isinstance(interactions, scipy.sparse.csr_matrix):\n",
    "        interactions = interactions.tocsr()\n",
    "    \n",
    "    ndcg = []\n",
    "    for user_id in range(interactions.shape[0]):  \n",
    "        scores = model.predict(user_id, np.arange(interactions.shape[1]), item_features=item_features)\n",
    "        true_items = interactions[user_id].toarray().flatten()  \n",
    "        ranked_items = np.argsort(-scores)[:k]\n",
    "   \n",
    "        dcg = sum(\n",
    "            (true_items[item] / np.log2(rank + 2))\n",
    "            for rank, item in enumerate(ranked_items)\n",
    "            if true_items[item] > 0\n",
    "        )\n",
    "      \n",
    "        sorted_true_items = np.sort(true_items)[::-1][:k]\n",
    "        idcg = sum(\n",
    "            (rel / np.log2(rank + 2))\n",
    "            for rank, rel in enumerate(sorted_true_items)\n",
    "            if rel > 0\n",
    "        )\n",
    "        ndcg.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "    return np.mean(ndcg)\n",
    "\n",
    "\n",
    "ndcg_at_5_train = calculate_ndcg(model, train, k=5, item_features=item_features)\n",
    "ndcg_at_10_train = calculate_ndcg(model, train, k=10, item_features=item_features)\n",
    "\n",
    "ndcg_at_5_test = calculate_ndcg(model, test, k=5, item_features=item_features)\n",
    "ndcg_at_10_test = calculate_ndcg(model, test, k=10, item_features=item_features)\n",
    "\n",
    "print(f\"Train nDCG@5: {ndcg_at_5_train:.4f}, Train nDCG@10: {ndcg_at_10_train:.4f}\")\n",
    "print(f\"Test nDCG@5: {ndcg_at_5_test:.4f}, Test nDCG@10: {ndcg_at_10_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
